{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derin Öğrenme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yapay sinir ağları olarak adlandırılan algoritmalarla ilgili makine öğreniminin bir alt dalıdır.\n",
    "Amaç insan beynini takilt etmek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tarihsel Gelişim\n",
    "###### Sibernetik (1940-1960)\n",
    "        biyolojik nöronu taklit etme girişimiyle insan beyninin nasıl örendiğini fikrine dayanan en eski model\n",
    "        kullanıcı tarafından verilen input ve weight değerlerine göre doğru/yanlış çıktısı veriyordu.\n",
    "        Çıktısı lineerdi ve Ml ile aynıydı daha kompleks yapı gerekiyordu\n",
    "        Sonra weightin otomatik öğrenecek olan Perceptron geliştirildi\n",
    "###### Connectivism(1980-1990)\n",
    "        Yapay sinir ağı bu zamanda tanıtıldı\n",
    "        3 katman var (input-hidden-output)\n",
    "        hidden katmanının amacı çeşitli matematiksel işlemlerle daha karmaşık output oluşturuluyor\n",
    "###### Derin Öğrenme(2006-)\n",
    "        Deep Belief Networku eğittiler ve daha farklı ağlar elde ediliyor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yapay Sinir Ağları\n",
    "        Birbirine bağlı nöronlara sahip bilgi işlem sistemleridir\n",
    "        input layer-> veriyi beslediğimiz katman\n",
    "        Hidden layer -> ilk başta lineer gelen değeri  non lineer hale getirmek için aktivasyon fonksiyonları kullanıyoruz\n",
    "        katman artarsa karmaşıklık artar\n",
    "        Output layer\n",
    "        \n",
    "        \n",
    "        Feed forward ve back propagation\n",
    "        ilk başta weight değerleri rastgele verildiği için modelin doğruluğundan emin olamıyoruz\n",
    "        en son çıktıya geldikten sonra back propagation yaparak her katmanda hatayı en aza indirgeyecek\n",
    "        şekilde weight değerlerini düzeltiyoruz\n",
    "        cpu gpu gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured data da geliştrime süresi daha uzun olduğu için tercih edilmez. Unstructred data(görüntü, ses vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nöron(Perceptron)\n",
    "\n",
    "Ii*Wi toplamı aktivasyon fonksiyonlarından geçerek output çıkarıyor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lojistik regresyon matematiği:\n",
    "\n",
    "Lineer fonksiyon sigmoid aktivasyon fonksiyonuna sokularak 0 ile 1 arasına sıkıştırılır ve ikili sınıflandırma\n",
    "problemleri çözülebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yapay sinir ağlarının komponentleri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### weight\n",
    "her bir nöron değerinin katsayılarıdır.\n",
    "###### bias\n",
    "aktivasyon fonksiyonunun verilere daha iyi uyması için sola sağa kaydırılması\n",
    "###### input\n",
    "input verisinin shape değeri ve yapay sinir ağının input katmanının shape değeri aynı olmalıdır\n",
    "\n",
    "**yapay sinir ağının input katmanı verileri rank 1 tensör olarak kabul eder veriler daha fazla boyuta sahipse rank 1 tensör e\n",
    "indirgenmelidir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_list = np.array([-40, -10, 0, 8, 15, 22, 38,50])\n",
    "fahrenheit_list = np.array([-40, 14, 32, 46, 59, 72, 100,122])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainValidateModel():\n",
    "    model=keras.Sequential()\n",
    "    model.add(Dense(1)) #tek nörünluk oluşturuyoruz\n",
    "    model.compile(loss='mean_squared_error',optimizer=keras.optimizers.Adam(0.1)) #learning rate\n",
    "    model.fit(celsius_list,fahrenheit_list,epochs=500, verbose=False)#bütün modelden sonra çıkıtoy veriyor false yapınca\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TrainValidateModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120.14152]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(np.array([50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.8295106]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([28.322477], dtype=float32)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.8690722]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([26.687906], dtype=float32)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#50 eklendikten sonra \n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Veri türleri ve Tensörler\n",
    "\n",
    "Skaler-> Tekil değere sahip verilerdir. Rank 0 Tensordur\n",
    "\n",
    "Vektör-> Tek boyutlu koleksiyon  verisidir. Rank 1 Tensordur\n",
    "\n",
    "Matrix-> İki boyutlu koleksiyon  verisidir. Rank 2 Tensordur\n",
    "\n",
    "Matrix-> İkiden çok boyutlu koleksiyon  verisidir. Rank >2 Tensordur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yapay sinir ağları nasıl çalışır?\n",
    "   ###### Forward Propagation\n",
    "       İnput verimiz sinir ağına verilir. weight ve bias değerleri rastgele verilir\n",
    "       sonuçlar aktivasdyon fonskiyonuna sokulur ve bias değeri ile kıyaslanır\n",
    "   ###### Back Propagation\n",
    "        Çıkan değer gerçek değer ile kıyaslanıp hata hesaplanır. hata değerleri ile gradyanlar hesaplanılır\n",
    "        Gradyanlar ile weightlar yeniden hesaplanır"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vanishing Gradient Problemi\n",
    "    Gradyanlar son katmandan ilk katmana doğru bulunur. Zincir kuralına göre ilk katmana gidene kadar\n",
    "    bu değerler 0 a yapklaşır ve bir şey üretemeyiz\n",
    "    Eğer aktivasyon fonksiyonumuzun çıktısı her zaman 0 çıktısı üretiyorsa, geri yayılım algoritması ile ağırlıkları güncelleyemeyiz.\n",
    "    Genil ve derin ağlar her katmanda büyük çıktılar üretilmesine neden olmaktadır. Sigmoid aktivasyon fonksiyonu kullanılarak dizayn edilmiş derin bir ağ gradyanın kaybolması ya da patlaması problemini doğuracaktır\n",
    "    \n",
    "##### Relu \n",
    " ReLU olarak adlandırılan çok basit bir aktivasyon fonksiyonunun ortaya atılması baharı getirmiştir. Bu fonksiyon pozitif girdiler için birim ya da özdeşlik fonksiyonu iken negatif girdiler için sıfır sonucunu üretmektedir.\n",
    " ###### Dying Relu Problemi\n",
    " Relu negatif inputların gradyanlarını 0 değerine eşitlediği için ağda çok fazla 0 üreten nöron varsa öğrenme\n",
    " sağlıklı olmayacaktır\n",
    " Bu noktada sızıntılı ReLU (Leaky ReLU) aktivasyon fonksiyonu her derdinize derman olabilir. Pozitif bölgelerde daha hızlı olmak kaydı fonksiyonun (sıfır noktası haricinde) her noktasında türevi alınabilmekte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aktivasyon Fonksiyonu\n",
    "Non lineeriteyi bozmak için kullanırız\n",
    "Softmax->Çok sayıda sınıflı bir siniflandırma probleminde çıktı olarak her sınıfa ait olasılığı döndürür\n",
    "\n",
    "##### Ağların optimizasyonu\n",
    "\n",
    "Underfitting\n",
    "\n",
    "Overfitting\n",
    "\n",
    "Regülarizasyon L1,L2\n",
    "\n",
    "Dropout\n",
    "\n",
    "    Nöronlar birlikte aktive olmaya başlarsa aralarındaki bağ artar. Nöronlar arası bağ kuvvetlendiği için model hep \n",
    "    o yollara odaklanıyor ve overfite sebep oluyor\n",
    "    Amaç Her epochda bu nöronlardan bazılarını kapatıp bu bağı kuvvetlendirmemeye çalışmak\n",
    "    \n",
    "Early Stopping\n",
    "\n",
    "Data Augmentation\n",
    "\n",
    "    Hali hazırda olan verilerin biraz değiştirilmiş kopyalarını kullanarak overfiti önlemeye çalışır.(mesela yüksek çözünürlüklü bir köpek fotoğrafının çözünürlüğünü düşürmek, ışığı değiştirmek vs)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer\n",
    "\n",
    "Momentum ile “schoastic gradient descent”in salınımları azaltılarak daha hızlı ve tutarlı bir optimizasyon algoritması oluşturulabilir.\n",
    "Optimizers are mathematical functions which are dependent on model’s learnable parameters i.e Weights & Biases. Optimizers help to know how to change weights and learning rate of neural network to reduce the losses.\n",
    "\n",
    "Loss functionu en aza indirmek ve accuracy i en üst seviyeye çıkarmaya çalışır\n",
    "\n",
    "Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent\n",
    "\n",
    "Advantages of Stochastic Gradient Descent\n",
    "Frequent updates of model parameter\n",
    "Requires less Memory.\n",
    "Allows the use of large data sets as it has to update only one example at a time.\n",
    "Disadvantages of Stochastic Gradient Descent\n",
    "The frequent can also result in noisy gradients which may cause the error to increase instead of decreasing it.\n",
    "High Variance.\n",
    "Frequent updates are computationally expensive\n",
    "\n",
    "RMS Prop\n",
    "    \n",
    "    uyarlanabilir bir öğrenme  hızı kullanır. Hata fazlayken adım büyük, küçükken küçüktür\n",
    "\n",
    "AdaGrad\n",
    "In all the algorithms that we discussed previously the learning rate remains constant. The intuition behind AdaGrad is can we use different Learning Rates for each and every neuron for each and every hidden layer based on different iterations.\n",
    "Advantages of AdaGrad\n",
    "Learning Rate changes adaptively with iterations.\n",
    "It is able to train sparse data as well.\n",
    "Disadvantage of AdaGrad\n",
    "If the neural network is deep the learning rate becomes very small number which will cause dead neuron problem.\n",
    "\n",
    "AdaDelta\n",
    "Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate and remove decaying learning rate problem. In Adadelta we do not need to set the default learning rate as we take the ratio of the running average of the previous time steps to the current gradient.\n",
    "Advantages of Adadelta\n",
    "The main advantage of AdaDelta is that we do not need to set a default learning rate.\n",
    "Disadvantages of Adadelta\n",
    "Computationally expensive\n",
    "\n",
    "Adam\n",
    "uyarlanabilir bir öğrenme oranı yöntemidir\n",
    "sinir ağının her ağırlığı için öğrenme oranını uyarlamak için 1. ve 2. gradyan anlarının tahminlerini kullanmasıdır\n",
    "Easy to implement\n",
    "Computationally efficient.\n",
    "Little memory requirements\n",
    "        \n",
    "\n",
    "\n",
    "How to choose optimizers?\n",
    "If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "Epoch\n",
    "\n",
    "Batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bilgisayarlı Görü ve Sayısal Görüntü İşleme\n",
    " Biligisayar algoritmaları kullanılırarak resimler üzerinde görüntü işleme gerçekleştirilmesidir\n",
    " Sisteme görüntü girer, özellikleri işlemek için bir feature extraction kullanılır sonra da  ml sistemi modeli eğitir\n",
    " \n",
    " Piksel\n",
    " \n",
    " Görüntüdeki bit noktanın sayısal değeridir. Bir görüntü tipik olarak bir piksel dizisi olarak temsil edilir.\n",
    " \n",
    " En ünlü renk alanlarından ikisi RGB(KIRMIZI,YEŞİL, MAVİ) VE HSV(TON, DOYGUNLUK, DEĞER) dir.\n",
    " \n",
    " Gürültü\n",
    " \n",
    " Resimlerin daha az net olması , kayıbı olması vs gibidir. Kendimiz de oluşturabiliriz her zaman temiz veri \n",
    " elimizde olmaz\n",
    " \n",
    " Feature Extraction\n",
    " \n",
    " Veri kümesindeki kaynak sayısını azaltmayı içerir boyut azaltmaya yarar\n",
    " \n",
    " Çok sayıda değişken ile analiz yapmak bellek açısından sıkıntı yaratır\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Neural Networks\n",
    "         \n",
    "   ##### 1) Giriş Katmanı\n",
    "         tek bir verkötüre şekillendirilir\n",
    "  \n",
    "  ##### 2) Stride (Adım)\n",
    "          Kayma başına kat edilen satır ve sütun sayısını ifade eder. Birden fazla öğeyi hareket ettirerek\n",
    "          sol üstten  başlayarak görüntüyü tarar\n",
    "  \n",
    "  ##### 3) Padding\n",
    "\n",
    "        adım adım görütünümüzü filtrelerken  görüntümüzün çevresindeki pikselleri kaybetme eğiliminde oluruz\n",
    "        görüntünün etrafına bizim için anlam ifade etmeyen şeyler ile doldurup gerçek görüntüden\n",
    "        elde edebileceğimiz maximum verimi almaya çalışırız.\n",
    "     \n",
    "  ##### Convulation Layer (Evrişim katmanı)\n",
    "\n",
    "        Görüntüyü tararken bizim de verebileceğimiz filtrelerle giriş boyutuna göre tarayıp öznitelik haritası\n",
    "        çıkartıyor\n",
    "        \n",
    "  ##### Pooling Katmanı\n",
    "      Non paramteric katman\n",
    "       Elde ettiğmiz öznitelik haritasından belirli özellikleri çıkarmak ve boyutunu azaltmak için kullanılır \n",
    "       Bazı parametrelerde yapılan hataları  hesaplama miktarlarını azaltmak için yapılan bir katman\n",
    "       Maks pooling-> adım ile veriyi tararken maksimum değerleri alır\n",
    "       Avg pooling-> adım ile veriyi tararken ortalama değerleri alır\n",
    "       \n",
    "  ##### Flattening Katmanı\n",
    "          \n",
    "        Poolingden sonra yine düzleştirlimeesini istiyoruz nundan sonra yapay sinir ağına girecek\n",
    "        \n",
    "  ##### Tam Bağlı Katman (Fully Connected Layer)\n",
    "  \n",
    "        Bir katmandaki tüm girdilerin bir sonraki katmanın her birimine bağlandığı katmanlardır\n",
    "        Standart bir yapay sinir ağı katmanıdır\n",
    "        En bütük avantajı yapıdan bağımısız olmasıdır. Girdi hakkında yapılması gereken özel varsayımları yoktur.\n",
    "        \n",
    "  ##### Bazı Aktivasyon Foksiyonları\n",
    "      So we want our activation function to not shift the gradient towards zero.\n",
    "      Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.\n",
    "      Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate.\n",
    "      As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.\n",
    "  \n",
    "        Sigmoid\n",
    "        This activation function is here only for historical reasons and never used in real models. It is computationally expensive, causes vanishing gradient problem and not zero-centred. This method is generally used for binary classification problems.\n",
    "        \n",
    "        sınıflandırma işlemlerinde evet hayır diyebileceğimiz durumlarda kullanılır.Binary çıktılarda kullanılır\n",
    "        \n",
    "        Softmax\n",
    "        Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models.\n",
    "        Çok sınıflı sınıflandırma algoritması olasılığını veriyor\n",
    "        \n",
    "        Relu\n",
    "        Sparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (having a non-zero output).\n",
    "    Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions\n",
    "    ReLUs aren’t without any drawbacks some of them are that ReLU is Non Zero centered and is non differentiable at Zero, but differentiable anywhere else.\n",
    "    Another problem we see in ReLU is the Dying ReLU problem where some ReLU Neurons essentially die for all inputs and remain inactive no matter what input is supplied, here no gradient flows and if large number of dead neurons are there in a Neural Network it’s performance is affected, this can be corrected by making use of what is called Leaky ReLU where slope is changed left of x=0 in above figure and thus causing a leak and extending the range of ReLU.\n",
    "    With Leaky ReLU there is a small negative slope, so instead of not firing at all for large gradients, our neurons do output some value and that makes our layer much more optimized too.\n",
    "        \n",
    "        \n",
    "        Tüm nörünları aynı anda aktive etmiyor. neagtif giriş değerini 0 a çevirir\n",
    "        \n",
    "        Elu\n",
    "        Exponential Linear Units are are used to speed up the deep learning process, this is done by making the mean activations closer to Zero, here an alpha constant is used which must be a positive number.\n",
    "        ELU have been shown to produce more accurate results than ReLU and also converge faster. ELU and ReLU are same for positive inputs, but for negative inputs ELU smoothes (to -alpha) slowly whereas ReLU smooths sharply.\n",
    "        \n",
    "        Tanh\n",
    "        If you compare it to sigmoid, it solves just one problem of being zero-centred.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ön Eğitilmiş CNN modelleri\n",
    "\n",
    "    daha önce büyük bir veri kümesinde eğitilmiş kayıtlı bir ağdır.Direkt modeli eğitilmiş olduğu gibi de kullanabiliriz \n",
    "    ya da belirli bir göreve özelleştirmek için transfer öğrenimini kullanırız\n",
    "    Xception\n",
    "    İnception v3\n",
    "    vgg16\n",
    "    vgg19\n",
    "    mobile net image net veri kümesi ile eğtilmiş modellerdir.\n",
    "    \n",
    "    \n",
    "###### LeNet 5 \n",
    "\n",
    "    İlk başarılı evrişimli sinir ağı. diğer modellerden farklı olarak\n",
    "    pooling aşamasında avg pooling kullanılır.\n",
    "    aktivasyon fonk olarak sigmoid ve hiperbolik tanjant fonk kullanıyor\n",
    "    \n",
    "###### VGG16\n",
    "    \n",
    "    Basit bir model. Evrişim katmanlarını 2 li ya da 3 lü kullanıyor. bitiminde softmax kullanıyor\n",
    "    138 milyon parametre hesabı yapıyor\n",
    "    \n",
    "###### ResNet 50\n",
    "\n",
    "       50 katman derinliğine sahip evrişimli sinir ağıdır\n",
    "       giriş boyutu 224*224 tür.\n",
    "       diğer ağlardan farklı olarak artık değerlerin sonraki katmanları besleyen blokların modele eklenmesiyle oluşur.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recurrent Neural Networks(RNN)\n",
    "\n",
    "    Döngüye sahip bir sinir ağıdır. Her girdiyi birbirnden bağımsız işler ve girdirler arasında durum bilgisi tutmazlar\n",
    "    Dizi mantığı ile ilerliyor. Videolarda özellikle kullanılır CNN ve RNN birleştilirerek videolar üzerinde çalışılır\n",
    "    Herhangi bir uzunluktaki veriyi işleyebilir fakat onara geri dönmek zordur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ###### Konvolüsyonel ağlarda kullanılan diğer teknikler\n",
    "  \n",
    "  ###### Data Augmentation\n",
    "          \n",
    "          Veriyi kendi elimizle arttırmak\n",
    "  \n",
    "  ###### Transfer Learning\n",
    "  \n",
    "      Bir porblemde öğrenilen özellikleri almak, ve bunları benzer bir problem üzerinde kullanmaktan oluşur.\n",
    "      Veri kümemimizin tam ölçekli bir modeli eğtimek için çok az veriye sahip olduğu durumlarda kullanılır.\n",
    "      \n",
    "          Önceden eğitilmiş modelden katmanlar alınır\n",
    "          Gelecekteki eğitim turlarında bilgiyi yok etmemek için  katmanlar donduruluyor (weight vs kaybetmemek adına)\n",
    "          Dondurulmuş katmanların üzerine yeni katmanlar eklenebilir \n",
    "          Sonra yeni katmanlar eğitilir\n",
    "          Daha iyi bir başlangıç modeli\n",
    "          Daha yüksek öğrenme oranı \n",
    "          Daha yüksek doğruluk\n",
    "          Daha hızlı eğitim sunuyor\n",
    "  \n",
    "  \n",
    "  ###### One Shot Learning\n",
    "  \n",
    "          Her sınıf için bir ya da çok az örneğin verildiği bir model hazırlamak için kullanılır\n",
    "          Çoğunlukla nesne sınıflandırma problemiyle ilgilenir\n",
    "          \n",
    "          \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesne Tanıma\n",
    "    \n",
    "    Robotik,güvenlik,sağlık, savunma sanay, vs kullanılır\n",
    "   #### Nesne Tanıma Mimarileri\n",
    "       \n",
    "       R-CNN\n",
    "       Fast R-CNN\n",
    "       Faster R-CNN->\n",
    "       Region Proposal Network yapısını kullanarak bölgeler oluşturur\n",
    "       Her bölgede nesne olduğunu varsayarak tarafsız puanlar atar bu işlemleri yaparken yoğunluk renk \n",
    "       piksel sırası gibi krşterlere bakar\n",
    "       Relu aktivayon fok kullanarak özellik haritası çıkarır\n",
    "       \n",
    "       YOLO ->\n",
    "       Çok hızlıdır\n",
    "           Görüntüleri gridlere böler bunları teker teker işleyerek bütüne gidiyor ve nesneleri koordinatları ile tek seferde\n",
    "           belirleyebilir\n",
    "           \n",
    "       SSD-> SingleShotDedector\n",
    "       \n",
    "       Özellik haritasi çıkarıp sonra evrişim filtreleri uygular\n",
    "       yüksek doğruluğunun olmasının sebebi farklı boyutlara sahip birden çok kutu kullanılıyor\n",
    "       \n",
    "       sınırlayıcı kutu giderek azalan bir evrişim filtresi içerir\n",
    "       \n",
    "       RefineNet\n",
    "       RefineDet\n",
    "       \n",
    "   ##### Nesne Tanıma DataSetleri\n",
    "   \n",
    "       COCO\n",
    "       PASCAL VOC()\n",
    "  \n",
    "#### Görüntü Sınıflandırma\n",
    "        \n",
    "        MNIST\n",
    "        IMAGENET\n",
    "        CIFAR-10\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing\n",
    "    \n",
    "\n",
    "    Text classification\n",
    "    translation\n",
    "    summarization\n",
    "    dialogue\n",
    "    quesiton answering\n",
    "    named entity recoginiton\n",
    "   \n",
    "   \n",
    "   ##### Transformer Mimarisi\n",
    "       \n",
    "       önce RNN\n",
    "       inputu weight matrisi ile çarpıp  non linearity den geçirdikten sonra h1 oluşuyor yı çıktısını da softmax ile\n",
    "       output veriyor ama her adımda output vermesi sorunlu bu geliştirilmiş\n",
    "       \n",
    "       Backpropagation through time\n",
    "       BP yaparken zamanda geriye gitmiş oluruz. (vanishing gradience problem RNN büyük kusuru)\n",
    "       \n",
    "       Long Short Term Memory(LSTM)\n",
    "       RNN in bir alt türü transformerden önceki en dominant network\n",
    "       speech recognition(en yaygın kullanılan yer ),vanish gradienti çözer\n",
    "       \n",
    "       Transformer\n",
    "       \n",
    "       Transformer yukarıda görülen mimarilerde olan adımsallığı ortadan kaldırarak adımların paralel çalışmasını sağlıyor\n",
    "       \n",
    "       input embedding de kelimeleri sayıya ceviriyoruz\n",
    "       word embedding-> anlam ilişlkisini görebiliyoruz çok boyutlular (pca ile boyut düşebilir)\n",
    "       \n",
    "       positional encodingde kelimelerin sırasının öenmli olduğunu vurguluyoruz\n",
    "       rnn de sıra ile işlem yapıldığı için sıra kaybı yoktu.Burada çözüm için ilk elemana 0 sona 1 ortadakilere de uygun\n",
    "       sayılar eklenerek çözüm olabilirdi. Neural network dışarıdan gelen sayılara karşı çok hassas biraz ekstre durumlarda\n",
    "       işe yaramayabilir\n",
    "       sin coslu bir çözüm var https://www.youtube.com/watch?v=dichIcUZfOw\n",
    "       \n",
    "       Encoder-Decoder-> her şey okunuyor sonra  bilgi kapsülüne sokuluyor(sabit uzunluklu bir vektör) anlama göre generate ediliyor\n",
    "       Eğer çok uzun bi veri varsa veri kaybolabilir override olabilir  bu yüzden attention(her adımda kapsüle ulaşılabiliyor) geliştiriliyor\n",
    "       \n",
    "       attention->scaled dot product attention->\n",
    "       sıraya göre değil de anlama göre sonuç döndürür self attention (query*key/sqrt(d))*value\n",
    "       \n",
    "       Dropout-> Bazı neural networkler training sırasında çıkarıyor. inputa bağlı olmadan öğrenmeyi sağlıyor.\n",
    "       \n",
    "       Residual Connection-> input girdikten sonra çeşitli işlemlere uğrar. kopyasını alarak bu işlemlere uğramadan\n",
    "       işlemlerden sonraki aşamada 2sini beraber kullanma şansını yakalarız\n",
    "       \n",
    "       Complexity\n",
    "       \n",
    "           self attention O(n^2*d)\n",
    "           recurrent O(n*d^2)\n",
    "           convolutional O(k*n*d^2)\n",
    "           self attention restiricted O(r*n*d)\n",
    "           \n",
    "           n sequence length\n",
    "           d dimension\n",
    "           k kernel size of convolutionals\n",
    "           r negihboorhood restricted sself at.\n",
    "       \n",
    "           learning scheduler\n",
    "       \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
